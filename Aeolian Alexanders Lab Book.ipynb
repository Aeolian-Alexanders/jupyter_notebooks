{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aeolian Alexanders: Economic, Material, and Social Networks in Antiquity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supported by a NEH - Mellon Digital Publication Grant <img src=\"img/neh_seal.jpg\" alt=\"NEH Seal\" style=\"width: 200px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "import sqlalchemy\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.font_manager as font_manager\n",
    "import matplotlib\n",
    "import argparse\n",
    "import numpy as np\n",
    "import matplotlib.font_manager as font_manager\n",
    "import json\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "import folium\n",
    "from folium import plugins\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "dlopen(/opt/anaconda3/lib/python3.7/site-packages/fiona/ogrext.cpython-37m-darwin.so, 2): Library not loaded: @rpath/libpoppler.76.dylib\n  Referenced from: /opt/anaconda3/lib/libgdal.20.dylib\n  Reason: image not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-41f4e635a102>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgeopandas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnetworkx\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcommunity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcm\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolors\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNormalize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/geopandas/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgeopandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_points_from_xy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpoints_from_xy\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgeopandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mread_file\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgeopandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mread_postgis\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgeopandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msjoin\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/geopandas/io/file.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mfiona\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgeopandas\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGeoDataFrame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGeoSeries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/fiona/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"PATH\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"PATH\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\";\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlibdir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mfiona\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBytesCollection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCollection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfiona\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrvsupport\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msupported_drivers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfiona\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mensure_env_with_credentials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEnv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/fiona/collection.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfiona\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvfs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mfiona\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mogrext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mItemsIterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKeysIterator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfiona\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mogrext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWritingSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfiona\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mogrext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbuffer_to_virtual_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_virtual_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGEOMETRY_TYPES\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: dlopen(/opt/anaconda3/lib/python3.7/site-packages/fiona/ogrext.cpython-37m-darwin.so, 2): Library not loaded: @rpath/libpoppler.76.dylib\n  Referenced from: /opt/anaconda3/lib/libgdal.20.dylib\n  Reason: image not found"
     ]
    }
   ],
   "source": [
    "import geopandas\n",
    "import networkx as nx\n",
    "import community\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.colors import Normalize\n",
    "import geopandas as gpd\n",
    "import contextily as ctx\n",
    "from pygraphviz import *\n",
    "from networkx.algorithms import bipartite\n",
    "from folium.features import DivIcon\n",
    "from natsort import natsorted\n",
    "import sqlite3\n",
    "\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine \n",
    "cnx = create_engine('sqlite:///aeolian_alexanders.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data structures used in the application. You can define any event and year soan following these models\n",
    "\n",
    "third_syrian_war = {\n",
    "    \"start\":-246,\n",
    "    \"end\":-241\n",
    "}\n",
    "\n",
    "war_of_antiochos = {\n",
    "    \"start\":-192,\n",
    "    \"end\":-188\n",
    "}\n",
    "\n",
    "# In this case the Attalids are the focus of the study, so the dates of the monarchs are important\n",
    "attalid_rulers = []\n",
    "\n",
    "Philetaerus = {'start_date': -282, 'end_date':-263}\n",
    "attalid_rulers.append(Philetaerus)\n",
    "EumenesI = {'start_date': -263, 'end_date':-241}\n",
    "attalid_rulers.append(EumenesI)\n",
    "AttalusI = {'start_date': -241, 'end_date':-197}\n",
    "attalid_rulers.append(AttalusI)\n",
    "EumenesII = {'start_date': -197, 'end_date':-160}\n",
    "attalid_rulers.append(EumenesII)\n",
    "AttalusII = {'start_date': -160, 'end_date':-138}\n",
    "attalid_rulers.append(AttalusII)\n",
    "AttalusIII = {'start_date': -138, 'end_date':-133}\n",
    "attalid_rulers.append(AttalusIII)\n",
    "\n",
    "#Polities are defined by their Pleiades IDs and whatever title you wish to give them\n",
    "\n",
    "polities = []\n",
    "polity_1 = {'title': 'Attalid_Kingdom','pid':550812}\n",
    "polities.append(polity_1)\n",
    "polity_2 = {'title': 'Seleucid_Kingdom','pid':45635759}\n",
    "polities.append(polity_2)\n",
    "polity_3 = {'title': 'Ptoemaic_Kingdom','pid':463803480}\n",
    "polities.append(polity_3)\n",
    "polity_4 = {'title': 'Lysimachus_Kingdom','pid':501458}\n",
    "polities.append(polity_4)\n",
    "\n",
    "# mints are defined by their Pleiades IDs; this could easily be changed to Nomisma IDs depending on your data setup\n",
    "mint = {\n",
    "    \n",
    "    'temnos': '550908',\n",
    "    'myrina': '550756',\n",
    "    'kyme': '550506'\n",
    "}\n",
    "\n",
    "# attribution for the map\n",
    "attr = \"\"\"\n",
    "Tiles &copy; <a href='http://mapbox.com/' target='_blank'>MapBox</a> |\n",
    " Data &copy; <a href='http://www.openstreetmap.org/' target='_blank'>OpenStreetMap</a> and contributors, CC-BY-SA |\n",
    " Tiles and Data &copy; 2020 <a href='http://www.awmc.unc.edu' target='_blank'>AWMC</a>\n",
    " <a href='http://creativecommons.org/licenses/by-nc/3.0/deed.en_US' target='_blank'>CC-BY-NC 3.0</a>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linReScale (oldmax, oldmin, newmax, newmin, oldnumber):\n",
    "    newnumber = (oldnumber - oldmin) / (oldmax - oldmin) * (newmax - newmin) + newmin\n",
    "    return newnumber\n",
    "\n",
    "def makeNetColor (row, my_colors):\n",
    "    netColor = matplotlib.colors.to_hex(my_colors(row['partition']))\n",
    "    #netColor = my_colors(row['partition'])\n",
    "    return netColor\n",
    "\n",
    "def makePleiadesGeoDataFromList(inputfile, id_column, outputname):\n",
    "    pleaidesdf = pd.DataFrame()\n",
    "    pleaidesdf = pd.read_csv(inputfile)  \n",
    "    pleiadesIdList = pleaidesdf['pid'].tolist()\n",
    "    sql = 'SELECT id, title, geom from places where ' + ' or '.join(('id = ' + str(n) for n in pleiadesIdList))\n",
    "    pleiadesGeodf = gpd.GeoDataFrame.from_postgis(sql, cnx, geom_col='geom' )\n",
    "    pleiadesGeodf.to_file(outputname, driver=\"GeoJSON\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### createmapvis makes our basic map\n",
    "def createmapvis():\n",
    "    \n",
    "    m = folium.Map(\n",
    "        location=[40.58058, 36.29883], \n",
    "        zoom_start=5,\n",
    "        zoom_control=False,\n",
    "        tiles='https://api.mapbox.com/v4/isawnyu.map-knmctlkh/{z}/{x}/{y}.png?access_token=pk.eyJ1IjoiaXNhd255dSIsImEiOiJja2FoNTRlMXIwOXYzMnpsbGJldGhyMjFqIn0.aDYskzcbh5inAn5JISgLyQ',\n",
    "        API_key='pk.eyJ1IjoiaXNhd255dSIsImEiOiJja2FoNTRlMXIwOXYzMnpsbGJldGhyMjFqIn0.aDYskzcbh5inAn5JISgLyQ',\n",
    "        name='AWMC Base',\n",
    "        attr= attr)\n",
    "    \n",
    "    return m\n",
    "\n",
    "\n",
    "\n",
    "#This produces a \"traditional\" coin catalog. This can be sorted if desired.\n",
    "def coinCatalogWorking (mint):\n",
    "    sql = \"\"\"\n",
    "    SELECT coinid AS id,\n",
    "    typeid AS type,\n",
    "    obvdie AS obverse,\n",
    "    revdie AS reverse,\n",
    "    weight,\n",
    "    rotation,\n",
    "    size,\n",
    "    title\n",
    "    FROM all_coins\n",
    "    where mint = '{mint}' AND obvdie !='' and revdie !=''\n",
    "    ORDER BY type, obverse, reverse\n",
    "    \"\"\".format(mint = mint)\n",
    "    \n",
    "    dfCoinCat = pd.read_sql_query(sql, cnx)\n",
    "    dfCoinCat[\"weight\"] = pd.to_numeric(dfCoinCat[\"weight\"])\n",
    "\n",
    "    return dfCoinCat\n",
    "\n",
    "#This function takes a mint, then creates the necessary .json files to put it into a d3js visualization using\n",
    "#our html templates\n",
    "\n",
    "def makeHtmlJson (mint):\n",
    "    revObvDf = pd.DataFrame()\n",
    "    obvTypeDf = pd.DataFrame()\n",
    "    revTypeDf = pd.DataFrame()\n",
    "    coinTypeDf = pd.DataFrame()\n",
    "    coinObvDf = pd.DataFrame()\n",
    "    coinRevDf = pd.DataFrame()\n",
    "    \n",
    "    coinNodesDf = pd.DataFrame()\n",
    "    \n",
    "    revObvDf = coinEdges('revdie', 'obvdie', mint)\n",
    "    obvTypeDf = coinEdges('obvdie', 'typeid', mint)\n",
    "    revTypeDf = coinEdges('revdie', 'typeid', mint)\n",
    "    coinTypeDf = coinEdges('id', 'typeid', mint)\n",
    "    coinObvDf = coinEdges('id', 'obvdie', mint)\n",
    "    coinRevDf = coinEdges('id', 'revdie', mint)\n",
    "    \n",
    "    coinNodesDf = coinNodes(mint)\n",
    "    \n",
    "    finalJson = '{\"nodes\":' + coinNodesDf.to_json(orient='records') + ','\n",
    "    finalJson = finalJson + '\"type_links\":' + coinTypeDf.to_json(orient='records') + ','\n",
    "    finalJson = finalJson + '\"coin_obverse_links\":' + coinObvDf.to_json(orient='records') + ','\n",
    "    finalJson = finalJson + '\"coin_reverse_links\":' + coinRevDf.to_json(orient='records') + ','\n",
    "    finalJson = finalJson + '\"coin_links\":' + revObvDf.to_json(orient='records') + ','\n",
    "    finalJson = finalJson + '\"obverse_type_links\":' + obvTypeDf.to_json(orient='records') + ','\n",
    "    finalJson = finalJson + '\"reverse_type_links\":' + revTypeDf.to_json(orient='records') + '}'\n",
    "    \n",
    "    return finalJson\n",
    "\n",
    "#This is a helper function for makeHtmlJson\n",
    "def coinEdges(source, target, mint):\n",
    "    sql = \"\"\"\n",
    "    SELECT {source} AS source, {target} AS target, COUNT ({source}) AS weight\n",
    "    FROM all_coins\n",
    "    where mint = '{mint}' AND obvdie !='' and revdie !=''\n",
    "    GROUP BY source, target\n",
    "    ORDER BY source\n",
    "    \"\"\".format(mint = mint, source = source, target = target)\n",
    "    \n",
    "    dfObvRev = pd.read_sql_query(sql, cnx)\n",
    "    \n",
    "    return dfObvRev\n",
    "\n",
    "#This is a helper function for makeHtmlJson\n",
    "def coinNodes(mint):\n",
    "    dfcoinCat = pd.DataFrame()\n",
    "    dfobvCat = pd.DataFrame()\n",
    "    dftypeCat = pd.DataFrame()\n",
    "\n",
    "    projectUri = ''\n",
    "    sql = \"\"\"\n",
    "    SELECT id, title, concat('{projectUri}', id) AS uri, 'coin' AS kind FROM all_coins \n",
    "    where mint = '{mint}' AND obvdie !='' and revdie !=''   \n",
    "    \"\"\".format(mint = mint, projectUri = projectUri)\n",
    "    dfcoinCat = pd.read_sql_query(sql, cnx)\n",
    "    \n",
    "    sql = \"\"\"\n",
    "    SELECT DISTINCT obvdie AS id, obvdie AS title, concat('{projectUri}', obvdie) AS uri, 'obverse' AS kind FROM all_coins \n",
    "    where mint = '{mint}' AND obvdie !='' and revdie !=''   \n",
    "    \"\"\".format(mint = mint, projectUri = projectUri)\n",
    "    \n",
    "    dfobvCat = pd.read_sql_query(sql, cnx)\n",
    "    dfcoinCat = dfcoinCat.append(dfobvCat, ignore_index = True) \n",
    "    \n",
    "    sql = \"\"\"\n",
    "    SELECT DISTINCT revdie AS id, obvdie AS title, concat('{projectUri}', revdie) AS uri, 'reverse' AS kind FROM all_coins \n",
    "    where mint = '{mint}' AND obvdie !='' and revdie !=''   \n",
    "    \"\"\".format(mint = mint, projectUri = projectUri)\n",
    "    \n",
    "    dfrevCat = pd.read_sql_query(sql, cnx)\n",
    "    dfcoinCat = dfcoinCat.append(dfrevCat, ignore_index = True) \n",
    "\n",
    "    sql = \"\"\"\n",
    "    SELECT DISTINCT typeid AS id, typeid AS title, concat('{projectUri}', typeid) AS uri, 'type' AS kind FROM all_coins \n",
    "    where mint = '{mint}' AND obvdie !='' and revdie !=''   \n",
    "    \"\"\".format(mint = mint, projectUri = projectUri)\n",
    "\n",
    "    dftypeCat = pd.read_sql_query(sql, cnx)\n",
    "    dfcoinCat = dfcoinCat.append(dftypeCat, ignore_index = True)\n",
    "    \n",
    "    return dfcoinCat\n",
    "\n",
    "#this function displays hoards associated with a chosen mint in a catalog. \n",
    "def hoardsDisplay (mint, startdate, enddate, buffer):\n",
    "    mintUri = 'https://pleiades.stoa.org/places/' + mint\n",
    "    sd = startdate - buffer;\n",
    "    ed = enddate - buffer;\n",
    "    \n",
    "    sql = \"\"\"\n",
    "        SELECT aa_hoards.hoard_id as id,\n",
    "        MAX(aa_hoards.title) AS title,\n",
    "        SUM(aa_mints.count) AS count,\n",
    "        MAX(aa_mints.denomination) AS denomination,\n",
    "        MAX(aa_mints.type) AS type,\n",
    "        MAX(aa_hoards.b_start_date) AS burial_start,\n",
    "        MAX(aa_hoards.b_end_date) AS burial_end,\n",
    "        MAX(aa_hoards.contents) AS contents,\n",
    "        MAX(aa_hoards.ex_start_date) AS discovered_at,\n",
    "        MAX(aa_hoards.location_uri) AS location_uri\n",
    "        FROM\n",
    "        aa_mints\n",
    "        LEFT JOIN\n",
    "        aa_hoards\n",
    "        ON\n",
    "        aa_mints.hoard = aa_hoards.hoard_id\n",
    "        WHERE \n",
    "        CAST(b_start_date as decimal) >= {sd} and CAST(b_end_date as decimal) <= {ed}\n",
    "        AND\n",
    "        aa_hoards.hoard_id NOT IN (SELECT cross_reference FROM aa_parent_child)\n",
    "        AND\n",
    "        aa_hoards.hoard_id IN (SELECT aa_mints.hoard from aa_mints WHERE aa_mints.mint_uri = '{mintUri}')\n",
    "        AND\n",
    "        aa_mints.mint_uri = '{mintUri}'\n",
    "        GROUP BY aa_hoards.hoard_id, aa_mints.mint_uri\n",
    "        ORDER BY burial_start; \n",
    "    \"\"\".format(sd = sd, ed = ed, mintUri=mintUri)\n",
    "    dfHoardCat = pd.read_sql_query(sql, cnx)\n",
    "    \n",
    "    return dfHoardCat\n",
    "\n",
    "#This function maps hoards which are associated with a selected mint\n",
    "def mintHoardMapper (hoarddf, nodeAttribute, mint, nodeMinSize, nodeMaxSize, m):\n",
    "    \n",
    "    sql = \"\"\"\n",
    "    SELECT aa_locations.id AS location_uri, aa_locations.lat, aa_locations.lon FROM aa_locations;\n",
    "    \"\"\"\n",
    "    locdf = pd.DataFrame()\n",
    "    locdf = pd.read_sql_query(sql, cnx)\n",
    "    \n",
    "    finaldf = hoarddf[(hoarddf['location_uri']!=\"null\")]\n",
    "    finaldf = finaldf[(finaldf['location_uri']!=\"\")]\n",
    "    finaldf = finaldf.merge(locdf, how='left')\n",
    "    \n",
    "    finaldf['lat'] = pd.to_numeric(finaldf['lat'])\n",
    "    finaldf['lon'] = pd.to_numeric(finaldf['lon'])\n",
    "    finaldf['count'] = pd.to_numeric(finaldf['count'])\n",
    "    \n",
    "    oldmax = hoarddf[nodeAttribute].max()\n",
    "    oldmin = hoarddf[nodeAttribute].min()\n",
    "    \n",
    "    finaldf['size'] = linReScale(oldmax, oldmin, nodeMaxSize, nodeMinSize, finaldf['count'])\n",
    "\n",
    "    \n",
    "    for index, row in finaldf.iterrows():\n",
    "        if float(row['lat']) < 9999999:\n",
    "            popupText = '{title}'.format(title = row['title'])\n",
    "            folium.CircleMarker(location = [row['lat'], row['lon']],\n",
    "                                popup = popupText,\n",
    "                                radius = row['size'],\n",
    "                                fill_color='black', \n",
    "                                color = 'black', \n",
    "                                fill_opacity=0.7,\n",
    "                                weight = .5\n",
    "                               ).add_to(m)\n",
    "    # now to put in the mint\n",
    "    \n",
    "    sql = \"\"\"\n",
    "    SELECT places.id, places.title, places.reprlat, places.reprlong\n",
    "    FROM places\n",
    "    WHERE id = {mint};\n",
    "    \"\"\".format(mint = mint)\n",
    "    \n",
    "    mintdf = pd.DataFrame()\n",
    "    mintdf = pd.read_sql_query(sql, cnx)\n",
    "    title = mintdf['title']\n",
    "    folium.Marker([mintdf['reprlat'], mintdf['reprlong']], popup='{title}'.format(title = title)).add_to(m)\n",
    "    \n",
    "    return m\n",
    "\n",
    "#this displays hoards as an ego network around a chosen mint\n",
    "def hoardsEgoDisplay (mint, startdate, enddate, buffer):\n",
    "    sd = startdate - buffer;\n",
    "    ed = enddate - buffer;\n",
    "    mintUri = 'https://pleiades.stoa.org/places/' + mint\n",
    "    sql = \"\"\"\n",
    "        SELECT aa_hoards.hoard_id,\n",
    "        MAX(aa_hoards.title) AS hoard_title,\n",
    "        MAX(aa_mints.mint) AS mint_title,\n",
    "        mint_uri,\n",
    "        SUM(aa_mints.count) AS count,\n",
    "        MAX(aa_hoards.location_uri) AS location_uri\n",
    "        FROM\n",
    "        aa_mints\n",
    "        LEFT JOIN\n",
    "        aa_hoards\n",
    "        ON\n",
    "        aa_mints.hoard = aa_hoards.hoard_id\n",
    "        WHERE \n",
    "        CAST(b_start_date as decimal) >= {sd} and CAST(b_end_date as decimal) <= {ed}\n",
    "        AND\n",
    "        aa_hoards.hoard_id NOT IN (SELECT cross_reference FROM aa_parent_child)\n",
    "        AND\n",
    "        aa_hoards.hoard_id IN (SELECT aa_mints.hoard from aa_mints WHERE aa_mints.mint_uri = '{mintUri}')\n",
    "        AND\n",
    "        mint_uri != ''\n",
    "        GROUP BY aa_hoards.hoard_id, mint_uri\n",
    "        ORDER BY mint_uri; \n",
    "    \"\"\".format(sd = sd, ed = ed, mintUri=mintUri)    \n",
    "    \n",
    "    dfHoardEgo = pd.read_sql_query(sql, cnx)\n",
    "    return dfHoardEgo\n",
    "\n",
    "#this displays a graph of all of the coin hoards connected with a mint\n",
    "def displayCoinEgoHoardGraph (coinHoardGraph, coinHoardGraphDict, maxNodeSize, MinNodeSize, maxEdgeSize, MinEdgeSize, figXsize, figYSize, nodes_df, FontSize):\n",
    "    \n",
    "    oldmax = int(max(coinHoardGraphDict.values()))\n",
    "    oldmin = int(min(coinHoardGraphDict.values()))\n",
    "    \n",
    "    for k, v in coinHoardGraphDict.items():\n",
    "        coinHoardGraphDict[k] = linReScale(oldmax, oldmin, maxNodeSize, MinNodeSize, v)\n",
    "    \n",
    "    edgeweights = [i['count'] for i in dict(coinHoardGraph.edges).values()]\n",
    "    oldmax = max(edgeweights)\n",
    "    oldmin = min(edgeweights)\n",
    "    \n",
    "    edgeweights = [linReScale(oldmax, oldmin, maxEdgeSize, MinEdgeSize, a) for a in edgeweights]\n",
    "    \n",
    "    labelcopy = pd.DataFrame()\n",
    "    labelcopy = nodes_df.copy()\n",
    "    labellist = labelcopy.filter(['mint_uri','title'])\n",
    "    labellistDict = pd.Series(labellist.title.values,index=labellist.mint_uri).to_dict()\n",
    "\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(figXsize,figYSize))\n",
    "    pos = nx.nx_agraph.graphviz_layout(coinHoardGraph, prog='neato')\n",
    "    \n",
    "    nx.draw_networkx_nodes(coinHoardGraph, pos, ax = ax, edgecolors = 'black', node_color=colors, cmap=my_colors, labels=True, node_size=[v * 50 for v in coinHoardGraphDict.values()])\n",
    "    nx.draw_networkx_edges(coinHoardGraph, pos, width=edgeweights, ax=ax)\n",
    "    nx.draw_networkx_labels(coinHoardGraph, pos, labellistDict, font_size=FontSize,font_color='black')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "#this creates dataframes from our graph data to use for maps and other \n",
    "def createMapDfs(dfEgoHoard, partition, graph, colormap):\n",
    "    smallhoarddf = dfEgoHoard.groupby( [\"location_uri\", \"hoard_id\"] ).count().reset_index()\n",
    "    sql = \"\"\"\n",
    "    SELECT aa_locations.id AS location_uri, aa_locations.title, aa_locations.lat, aa_locations.lon FROM aa_locations;\n",
    "    \"\"\"\n",
    "    \n",
    "    locdf = pd.DataFrame()\n",
    "    locdf = pd.read_sql_query(sql, cnx)\n",
    "    smallhoarddf = smallhoarddf.merge(locdf, how='left')\n",
    "    del smallhoarddf['hoard_title']\n",
    "    del smallhoarddf['mint_title']\n",
    "    del smallhoarddf['mint_uri']\n",
    "    del smallhoarddf['count']\n",
    "    smallhoarddf.rename(columns={'location_uri':'mint_uri'}, inplace=True)\n",
    "    smallhoarddf['title'] = smallhoarddf['hoard_id']\n",
    "    mapDf = pd.DataFrame(coinHoardGraph.nodes, columns =['mint_uri']) \n",
    "    partdf = pd.DataFrame(list(partition.items()),columns = ['mint_uri','partition'])\n",
    "    partdf['color'] = partdf.apply (lambda row: makeNetColor(row, my_colors), axis=1)\n",
    "    mapDf = mapDf.merge(partdf, how='left')\n",
    "    weightdf = pd.DataFrame(list(coinHoardGraph.degree(weight='weight')),columns = ['mint_uri','degree']) \n",
    "    mapDf = mapDf.merge(weightdf, how='left')\n",
    "    \n",
    "    sql = \"\"\"\n",
    "    SELECT aa_locations.id AS mint_uri, aa_locations.title, aa_locations.lat, aa_locations.lon \n",
    "    FROM aa_locations;\n",
    "    \"\"\"\n",
    "    \n",
    "    locdf = pd.DataFrame()\n",
    "    locdf = pd.read_sql_query(sql, cnx)\n",
    "    \n",
    "    mapDf = mapDf.merge(locdf, how='left')\n",
    "    partdf.rename(columns={'mint_uri':'hoard_id'}, inplace=True)\n",
    "    weightdf.rename(columns={'mint_uri':'hoard_id'}, inplace=True)\n",
    "    \n",
    "    smallhoarddf = smallhoarddf.merge(partdf, how='left')\n",
    "    smallhoarddf = smallhoarddf.merge(weightdf, how='left')\n",
    "    \n",
    "    del smallhoarddf['mint_uri']\n",
    "    smallhoarddf.rename(columns={'hoard_id':'mint_uri'}, inplace=True)\n",
    "    \n",
    "    mapDf = mapDf[mapDf['lat'].notna()]\n",
    "    \n",
    "    smallhoarddf['lat'] = pd.to_numeric(smallhoarddf['lat'])\n",
    "    smallhoarddf['lon'] = pd.to_numeric(smallhoarddf['lon'])\n",
    "    smallhoarddf['degree'] = pd.to_numeric(smallhoarddf['degree'])\n",
    "\n",
    "    mapDf['lat'] = pd.to_numeric(mapDf['lat'])\n",
    "    mapDf['lon'] = pd.to_numeric(mapDf['lon'])\n",
    "    mapDf['degree'] = pd.to_numeric(mapDf['degree'])\n",
    "\n",
    "    \n",
    "    dataframeResults['mintsDf'] = mapDf\n",
    "    dataframeResults['hoardsDf'] = smallhoarddf\n",
    "    \n",
    "    mapDf = mapDf.merge(smallhoarddf, how='outer')\n",
    "    mapDf = mapDf[(mapDf['lat']!=\"lat\")]\n",
    "    mapDf = mapDf[(mapDf['lat']!=\"\")]\n",
    "    \n",
    "    dataframeResults['mapDf'] = mapDf\n",
    "    \n",
    "    return dataframeResults\n",
    "\n",
    "#this function populates a feature group from a datatable, rescaleas it, then adds it to the map\n",
    "\n",
    "def addFeatureGrouptoMap(df, featuregroup, nodeMaxSize, nodeMinSize, nodeAttribute, nodeTitle, m):\n",
    "    \n",
    "    oldmax = df[nodeAttribute].max()\n",
    "    oldmin = df[nodeAttribute].min()\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        if float(row['lat']) < 9999999:\n",
    "            radius = linReScale(oldmax, oldmin, nodeMaxSize, nodeMinSize, row['degree'])\n",
    "            popupText = 'Title: {title}'.format(title = row[nodeTitle])\n",
    "            featuregroup.add_child(folium.CircleMarker(location = [row['lat'], row['lon']],\n",
    "                                popup = popupText,\n",
    "                                radius = radius,\n",
    "                                fill_color=row['color'], \n",
    "                                color = 'black', \n",
    "                                fill_opacity=0.7,\n",
    "                                weight = .5\n",
    "                               ))\n",
    "            m.add_child(featuregroup) \n",
    "\n",
    "def addFeatureLabelstoMap(df, nodeTitle, m, fontsize):\n",
    "    for index, row in df.iterrows():\n",
    "        if float(row['lat']) < 9999999:\n",
    "            folium.Marker(location = [row['lat'], row['lon']],\n",
    "                          icon=DivIcon(\n",
    "                              icon_size=(15,15),\n",
    "                              icon_anchor=(0,0),\n",
    "                              html='<div style=\"font-size: {fontsize}pt\">{label}</div>'.format(label = row[nodeTitle], fontsize = fontsize))\n",
    "                         ).add_to(m)\n",
    "\n",
    "#this function maps all of the hoards between two given years, partitions them, and returns a dataframe           \n",
    "def hoardsGroupMapper(startdate, enddate, buffer, mapcolorramp, featuregroup):\n",
    "    sql = coinNetworkSql(startdate, enddate, buffer)\n",
    "    dfmaptest = pd.DataFrame()\n",
    "    dfmaptest = pd.read_sql_query(sql, cnx)\n",
    "    mapgraph = nx.from_pandas_edgelist(dfmaptest, 'source', 'target', 'weight')\n",
    "    partition = community.best_partition(mapgraph)\n",
    "    modularity = community.modularity(partition, mapgraph)\n",
    "    dfmap = createMapDataframe(dfmaptest, mapgraph, partition, mapcolorramp)\n",
    "    return dfmap\n",
    "\n",
    "### This function creates the dataframe necessary for the map out of all of the various measurements, etc.\n",
    "def createMapDataframe(dataframe, graph, partition, color_map):\n",
    "    sql = \"\"\"\n",
    "        SELECT aa_locations.id AS source, \n",
    "        aa_locations.title, \n",
    "        aa_locations.lat, \n",
    "        aa_locations.lon, \n",
    "        aa_locations.geom FROM aa_locations;\n",
    "        \"\"\"\n",
    "    dataframe = pd.DataFrame()\n",
    "    dataframe = pd.read_sql_query(sql, cnx)\n",
    "    partdf = pd.DataFrame(list(partition.items()),columns = ['source','partition'])\n",
    "    dataframe = dataframe.merge(partdf, how='left')\n",
    "    weightdf = pd.DataFrame(list(graph.degree(weight='weight')),columns = ['source','weighted_degree']) \n",
    "    dataframe = dataframe.merge(weightdf, how='left')\n",
    "    degreedf = pd.DataFrame(list(graph.degree()),columns = ['source','degree']) \n",
    "    dataframe = dataframe.merge(degreedf, how='left')\n",
    "    dataframe = dataframe[dataframe.partition.notnull()]\n",
    "    columncolor = dataframe[['source', 'partition']] \n",
    "    \n",
    "    minima = min(columncolor['partition'].tolist())\n",
    "    maxima = max(columncolor['partition'].tolist())\n",
    "    norm = matplotlib.colors.Normalize(vmin=minima, vmax=maxima, clip=True)\n",
    "    mapper = cm.ScalarMappable(norm=norm, cmap=color_map)\n",
    "    \n",
    "    aDict = {}\n",
    "    #since for some reasin the color mapper is not callable within an iterated dataframe, we are going to do this the hard way\n",
    "    for index, row in dataframe.iterrows():\n",
    "        aDict[row['source']] = str(matplotlib.colors.to_hex(mapper.to_rgba(row['partition'])))\n",
    "    \n",
    "    colordf = pd.DataFrame(list(aDict.items()), columns=['source', 'color'])\n",
    "    dataframe = dataframe.merge(colordf, how='left')\n",
    "    \n",
    "    sql = \"\"\"\n",
    "    SELECT aa_locations.id AS location_uri, aa_locations.lat, aa_locations.lon FROM aa_locations;\n",
    "    \"\"\"\n",
    "    locdf = pd.DataFrame()\n",
    "    locdf = pd.read_sql_query(sql, cnx)\n",
    "\n",
    "\n",
    "    dataframe = dataframe.merge(locdf, how='left')\n",
    "    dataframe[[\"lat\", \"lon\"]] = dataframe[[\"lat\", \"lon\"]].apply(pd.to_numeric)\n",
    "    \n",
    "    return dataframe\n",
    "\n",
    "#this function grabs dynastic mint information from the ANS\n",
    "def dynastymintlocator(year_start, year_end, dynasty):\n",
    "    sql = \"\"\"\n",
    "        SELECT aa_mantis.mint, nomisma_mints.mint, nomisma_mints.pleiades, nomisma_mints.lat, nomisma_mints.long  \n",
    "        FROM aa_mantis\n",
    "        FULL OUTER JOIN nomisma_mints\n",
    "        ON\n",
    "        LOWER(aa_mantis.mint) = LOWER (nomisma_mints.label)\n",
    "        WHERE\n",
    "        aa_mantis.start_year BETWEEN {year_start} AND {year_end}\n",
    "        AND\n",
    "        aa_mantis.end_year BETWEEN {year_start} AND {year_end}\n",
    "        AND\n",
    "        LOWER(aa_mantis.dynasty) LIKE LOWER('%%{dynasty}%%')\n",
    "        GROUP BY\n",
    "        aa_mantis.mint,nomisma_mints.mint, nomisma_mints.pleiades, nomisma_mints.lat, nomisma_mints.long\n",
    "        ;\n",
    "        \"\"\".format(year_start = year_start,year_end = year_end,dynasty = dynasty)\n",
    "    dataframe = pd.DataFrame()\n",
    "    dataframe = pd.read_sql_query(sql, cnx)\n",
    "    geodf = geopandas.GeoDataFrame(dataframe, geometry=geopandas.points_from_xy(dataframe.long, dataframe.lat))\n",
    "    return geodf;\n",
    "\n",
    "#I want to map the dynasties slightly differently, so they get their own folium group\n",
    "def dynastyMintMapper(geodf, dyncolor, radius, featuregroup, m):\n",
    "    for index, row in geodf[geodf.lat.notnull()].iterrows():\n",
    "        featuregroup.add_child(\n",
    "            folium.CircleMarker(\n",
    "                location=[row['geometry'].xy[1][0], row['geometry'].xy[0][0]],\n",
    "                fill=True,\n",
    "                fill_color = dyncolor,\n",
    "                color = dyncolor,\n",
    "                opacity = 0.4,\n",
    "                fill_opacity=0.4,\n",
    "                radius = radius\n",
    "            ))\n",
    "    m.add_child(featuregroup)\n",
    "\n",
    "        \n",
    "\n",
    "    \n",
    "#A funvtion tying together the pgRouting \n",
    "def polityRouteMaker (df, df_id, source_table, ego_table, ego):\n",
    "    nodeList = []\n",
    "    # now we need to get the actual vector value  on our nodded route network for each of the recognized place entries in the table\n",
    "    for index, row in df.iterrows():\n",
    "        sql = pgrPointFinder(row[df_id], source_table)\n",
    "        results_set = cnx.execute(sql)\n",
    "        # ugly hardcoded for now; may need to make this more flexible in the future\n",
    "        # after the result is in, we add it to the list\n",
    "        for id in results_set:\n",
    "            nodeList.append(int(id[0]))\n",
    "    placerecord = cnx.execute(pgrPointFinder (ego, ego_table))\n",
    "    # some more ugly hard coding here; this should alwyas be the first result. I think/hope/etc...\n",
    "    id_count = placerecord.first()[0]\n",
    "    # now to get the pleiades ID and router match up\n",
    "    sql = politicalPgrMaker(id_count, nodeList)\n",
    "    # now to do the actual routing\n",
    "    df = gpd.GeoDataFrame.from_postgis(sql, cnx, geom_col='geom' )\n",
    "    return df\n",
    "\n",
    "#finds the closest point in our routes network to a given goegraphic point\n",
    "def pgrPointFinder (placeId, table):\n",
    "    sql = \"\"\"\n",
    "        SELECT closest_pt.id\n",
    "        FROM {table} a\n",
    "        CROSS JOIN LATERAL\n",
    "        (SELECT\n",
    "        id,\n",
    "        a.geom <-> b.the_geom AS dist\n",
    "        FROM routes_single_noded_vertices_pgr b\n",
    "        WHERE a.id = '{placeId}'\n",
    "        ORDER BY a.geom <-> b.the_geom\n",
    "        LIMIT 1) AS closest_pt;\n",
    "        \"\"\".format(placeId = placeId, table=table)\n",
    "    return sql\n",
    "\n",
    "\n",
    "#actually makes the routes from nodes and edges\n",
    "def politicalPgrMaker (target, connected_list):\n",
    "    sql = \"\"\"\n",
    "    SELECT\n",
    "    max(seq),\n",
    "    edge,\n",
    "    count(edge),\n",
    "    geom\n",
    "    FROM\n",
    "        pgr_dijkstra(\n",
    "        'SELECT id,\n",
    "        source,\n",
    "        target,\n",
    "        st_length(geom) AS cost\n",
    "        FROM routes_single_noded',\n",
    "        array{connected_list},\n",
    "        {target},\n",
    "        false) AS pt\n",
    "    JOIN routes_single_noded rd \n",
    "    ON pt.edge = rd.id\n",
    "    GROUP BY\n",
    "        edge,\n",
    "        geom;\n",
    "    \"\"\".format(target = target, connected_list = connected_list)\n",
    "    return sql\n",
    "\n",
    "#this function returns routes for a selected polity \n",
    "def ancientEmpireRoutes(polity, tableName, start_date, end_date):\n",
    "    #stripping out any non-pleaides ids, as by definition these will not show up\n",
    "    sql = \"\"\"\n",
    "    select places.id\n",
    "    from places JOIN {tableName}\n",
    "    ON places.id::varchar = {tableName}.source\n",
    "    WHERE target ='{polity}'\n",
    "    AND\n",
    "    places.reprlatlong IS NOT NULL\n",
    "    AND\n",
    "    ({tableName}.start_date::double precision <= {end_date})\n",
    "    AND\n",
    "    ({tableName}.end_date::double precision >= {start_date})\n",
    "    AND\n",
    "    {tableName}.type IN ('capitol', 'hegemon of', 'control by', 'founded by', 'garrisoned by', 'subordinate')\n",
    "    GROUP BY places.id\n",
    "    \"\"\".format(polity = polity, start_date=start_date, end_date = end_date, tableName = tableName)\n",
    "    df = pd.DataFrame()\n",
    "    df = pd.read_sql_query(sql, cnx)\n",
    "    return df\n",
    "\n",
    "#this function returns places for a selected polity \n",
    "def ancientEmpirePoints(polity, tableName, start_date, end_date):\n",
    "    #stripping out any non-pleaides ids, as by definition these will not show up\n",
    "    sql = \"\"\"\n",
    "    select places.id, places.title, places.geom\n",
    "    from places JOIN {tableName}\n",
    "    ON places.id::varchar = {tableName}.source\n",
    "    WHERE target ='{polity}'\n",
    "    AND\n",
    "    places.reprlatlong IS NOT NULL\n",
    "    AND\n",
    "    ({tableName}.start_date::double precision <= {end_date})\n",
    "    AND\n",
    "    ({tableName}.end_date::double precision >= {start_date})\n",
    "    AND\n",
    "    {tableName}.type IN ('capitol', 'hegemon of', 'control by', 'founded by', 'garrisoned by', 'subordinate')\n",
    "    GROUP BY places.id\n",
    "    \"\"\".format(polity = polity, start_date=start_date, end_date = end_date, tableName = tableName)\n",
    "    df = pd.DataFrame()\n",
    "    df = gpd.GeoDataFrame.from_postgis(sql, cnx, geom_col='geom' )\n",
    "    return df\n",
    "\n",
    "def obversetoreversecounter(mint):\n",
    "    sql = \"\"\"\n",
    "    Select obvdie AS obverse_die, count(distinct revdie) as number_of_reverse_dies\n",
    "    from\n",
    "    all_coins\n",
    "    where mint = '{mint}'\n",
    "    AND\n",
    "    obvdie IS NOT NULL\n",
    "    AND\n",
    "    obvdie != ''\n",
    "    AND\n",
    "    obvdie !='dupe'\n",
    "    AND\n",
    "    obvdie !='no_image'\n",
    "    group by obvdie\n",
    "    \"\"\".format(mint = mint)\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    df = pd.read_sql_query(sql, cnx)\n",
    "    \n",
    "    #we usually have mixed numbers and characters for naming conventions; this should arrange everythign nicely\n",
    "    df.index = df['obverse_die'].str.replace('[a-zA-Z_]+','').astype(float)\n",
    "    df = df.sort_index().reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def obversetoreversecounter(mint):\n",
    "    sql = \"\"\"\n",
    "    Select obvdie AS obverse_die, count(distinct revdie) as number_of_reverse_dies\n",
    "    from\n",
    "    all_coins\n",
    "    where mint = '{mint}'\n",
    "    AND\n",
    "    obvdie IS NOT NULL\n",
    "    AND\n",
    "    obvdie != ''\n",
    "    AND\n",
    "    obvdie !='dupe'\n",
    "    AND\n",
    "    obvdie !='no_image'\n",
    "    group by obvdie\n",
    "    \"\"\".format(mint = mint)\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    df = pd.read_sql_query(sql, cnx)\n",
    "    \n",
    "    #we usually have mixed numbers and characters for naming conventions; this should arrange everythign nicely\n",
    "    df.index = df['obverse_die'].str.replace('[a-zA-Z_]+','').astype(float)\n",
    "    df = df.sort_index().reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def obverseweightaverager(mint):\n",
    "    sql = \"\"\"\n",
    "    Select obvdie AS obverse_die, AVG(CAST(weight as decimal)) as average_weight\n",
    "    from\n",
    "    all_coins\n",
    "    where mint = '{mint}'\n",
    "    AND\n",
    "    obvdie IS NOT NULL\n",
    "    AND\n",
    "    obvdie != ''\n",
    "    AND\n",
    "    obvdie !='dupe'\n",
    "    AND\n",
    "    obvdie !='no_image'\n",
    "    group by obvdie\n",
    "    \"\"\".format(mint = mint)\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    df = pd.read_sql_query(sql, cnx)\n",
    "    \n",
    "    #we usually have mixed numbers and characters for naming conventions; this should arrange everythign nicely\n",
    "    df.index = df['obverse_die'].str.replace('[a-zA-Z_]+','').astype(float)\n",
    "    df = df.sort_index().reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def obversecoincounter(mint):\n",
    "    sql = \"\"\"\n",
    "    Select obvdie AS obverse_die, count(obvdie) AS number_of_coins\n",
    "    from\n",
    "    all_coins\n",
    "    where mint = '{mint}'\n",
    "    AND\n",
    "    obvdie IS NOT NULL\n",
    "    AND\n",
    "    obvdie != ''\n",
    "    AND\n",
    "    obvdie !='dupe'\n",
    "    AND\n",
    "    obvdie !='no_image'\n",
    "    group by obvdie\n",
    "    \"\"\".format(mint = mint)\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    df = pd.read_sql_query(sql, cnx)\n",
    "    \n",
    "    #we usually have mixed numbers and characters for naming conventions; this should arrange everythign nicely\n",
    "    df.index = df['obverse_die'].str.replace('[a-zA-Z_]+','').astype(float)\n",
    "    df = df.sort_index().reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def obverseToTypeList (mint):\n",
    "    sql = \"\"\"\n",
    "    Select obvdie AS obverse_die, group_concat(DISTINCT typeid) as price_types\n",
    "    from\n",
    "    all_coins\n",
    "    where mint = '{mint}'\n",
    "    AND\n",
    "    obvdie IS NOT NULL\n",
    "    AND\n",
    "    obvdie != ''\n",
    "    AND\n",
    "    obvdie !='dupe'\n",
    "    AND\n",
    "    obvdie !='no_image'\n",
    "    AND\n",
    "    typeid !=''\n",
    "    AND\n",
    "    typeid IS NOT NULL\n",
    "    group by obvdie\n",
    "    \"\"\".format(mint = mint)\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    df = pd.read_sql_query(sql, cnx)\n",
    "    df = sortMixedDatatable(df)\n",
    "\n",
    "    return df\n",
    "\n",
    "def obvinfochart(mint):\n",
    "    dfObverseReverseCount = pd.DataFrame()\n",
    "    dfObverseReverseCount = obversetoreversecounter(mint)\n",
    "    dfobvprice = pd.DataFrame()\n",
    "    dfobvprice = obverseToTypeList(mint)\n",
    "    dfobvavgweight = pd.DataFrame()\n",
    "    dfobvavgweight = obverseweightaverager(mint)\n",
    "    dfobvcoincount = pd.DataFrame()\n",
    "    dfobvcoincount = obversecoincounter(mint)\n",
    "    dfObvinfo = pd.DataFrame()\n",
    "    dfObvinfo = dfobvcoincount.copy()\n",
    "    dfObvinfo = dfObvinfo.merge(dfObverseReverseCount, how='left')\n",
    "    dfObvinfo = dfObvinfo.merge(dfobvavgweight, how='left')\n",
    "    dfObvinfo = dfObvinfo.merge(dfobvprice, how='left')\n",
    "    dfObvinfo.set_index('obverse_die', inplace=True)\n",
    "    return dfObvinfo\n",
    "\n",
    "def sortMixedDatatable (df):\n",
    "    #we usually have mixed numbers and characters for naming conventions; this should arrange everythign nicely\n",
    "    df.index = df['obverse_die'].str.replace('[a-zA-Z_]+','').astype(float)\n",
    "    df = df.sort_index().reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "#this function creates the big dataframe for all of the routes\n",
    "def createBigNetworkDf():\n",
    "    sql = \"\"\"\n",
    "    SELECT aa_mints.mint_uri as source, a.location_uri as target, aa_mints.mint as label, \n",
    "    aa_mints.count as weight, \n",
    "    CONCAT(LPAD((@a.b_start_date)::text, 4, '0'), ' ',start_stamp ) AS start_date,\n",
    "    CONCAT(LPAD((@a.b_end_date)::text, 4, '0'), ' ',end_stamp ) AS end_date\n",
    "    FROM aa_mints JOIN\n",
    "    (SELECT aa_hoards.hoard_id, aa_hoards.location_uri, aa_locations.geom, b_start_date,\n",
    "    CASE\n",
    "    WHEN  b_start_date::varchar LIKE '-%%' THEN 'BC'\n",
    "    ELSE 'AD'\n",
    "    END \n",
    "    AS start_stamp, \n",
    "    b_end_date,\n",
    "    CASE\n",
    "    WHEN  b_end_date::varchar LIKE '-%%' THEN 'BC'\n",
    "    ELSE 'AD'\n",
    "    END \n",
    "    AS end_stamp\n",
    "    FROM aa_hoards LEFT JOIN aa_locations \n",
    "    ON LOWER(aa_hoards.location_uri) = LOWER (aa_locations.id) \n",
    "    WHERE aa_hoards.hoard_id NOT IN (SELECT cross_reference from aa_parent_child)\n",
    "    AND b_start_date IS NOT NULL\n",
    "    ) a\n",
    "    \n",
    "    ON aa_mints.hoard = a.hoard_id\n",
    "    WHERE\n",
    "    aa_mints.mint_uri IS NOT NULL \n",
    "    AND aa_mints.mint_uri <> '' \n",
    "    AND a.location_uri IS NOT NULL \n",
    "    AND a.location_uri <> ''\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    df = pd.read_sql_query(sql, cnx)\n",
    "    df[\"weight\"] = pd.to_numeric(df[\"weight\"])\n",
    "    df[\"weight\"].fillna(1)\n",
    "    df[\"weight\"] = df[\"weight\"].replace(np.nan, 1, regex=True)\n",
    "    df[\"weight\"].replace(r'^\\s*$', 1, regex=True)\n",
    "    return df\n",
    "\n",
    "#this function inserts the dataframe into the database in the format required for QGIS\n",
    "def loadBigRouteFrame(df):\n",
    "    for index, row in df.iterrows():\n",
    "        sql = \"\"\"\n",
    "        SELECT '{mint}' as mint, '{hoard}' as hoard, '{weight}' as weight, '{start_date}' as start_date, '{end_date}' as end_date, ST_AsText(st_union (a.geom)) as geom FROM\n",
    "        (SELECT max(seq), edge, count(edge), geom\n",
    "        FROM pgr_dijkstra(\n",
    "        'SELECT id, source, target, st_length(geom) as cost FROM routes_single_noded',\n",
    "    (select array_agg(closest_pt.id)\n",
    "    from (SELECT q.mint_uri AS source, q.location_uri AS target, r.geom \n",
    "    from (select aa_mints.mint_uri, aa_hoards.hoard_id, aa_hoards.location_uri \n",
    "    FROM aa_mints JOIN aa_hoards \n",
    "    ON aa_mints.hoard = aa_hoards.hoard_id \n",
    "    WHERE aa_hoards.hoard_id NOT IN \n",
    "    (SELECT cross_reference from aa_parent_child) \n",
    "    AND \n",
    "    aa_mints.mint_uri <> '' \n",
    "    AND \n",
    "    aa_mints.mint_uri IS NOT NULL \n",
    "    AND \n",
    "    aa_hoards.location_uri <> '' \n",
    "    AND \n",
    "    aa_hoards.location_uri IS NOT NULL\n",
    "    ) q\n",
    "    JOIN\n",
    "    (SELECT aa_locations.geom, aa_locations.id from aa_locations)r \n",
    "    ON \n",
    "    q.location_uri = r.id \n",
    "    WHERE r.geom IS NOT NULL) a \n",
    "    CROSS JOIN LATERAL\n",
    "    (SELECT\n",
    "    id , \n",
    "    a.geom <-> b.the_geom as dist\n",
    "    FROM routes_single_noded_vertices_pgr b\n",
    "    WHERE a.source = '{mint}'\n",
    "    ORDER BY a.geom <-> b.the_geom \n",
    "    LIMIT 1) AS closest_pt), \n",
    "    (select closest_pt.id \n",
    "    from aa_locations a \n",
    "    CROSS JOIN LATERAL \n",
    "    (SELECT \n",
    "    id, \n",
    "    a.geom <-> b.the_geom as dist \n",
    "    FROM routes_single_noded_vertices_pgr b \n",
    "    WHERE a.id = '{mint}' \n",
    "    ORDER BY a.geom <-> b.the_geom \n",
    "    LIMIT 1) AS closest_pt\n",
    "    ), false\n",
    "    ) as pt \n",
    "    JOIN routes_single_noded rd ON pt.edge = rd.id \n",
    "    GROUP BY edge, geom) a; \n",
    "    \"\"\".format(mint = row['source'], hoard = row['target'], weight = row['weight'], start_date =row['start_date'] , end_date = row['end_date'] )\n",
    "    df2 = pd.DataFrame()\n",
    "    df2 = pd.read_sql(sql, cnx)\n",
    "    sqlInsert = \"\"\"\n",
    "        INSERT INTO aa_coin_routes(mint, hoard, weight, start_date, end_date, geom)\n",
    "        VALUES('{mint}', '{hoard}', {weight}, '{start_date}', '{end_date}', st_multi(ST_GeomFromText('{geom}', 4326)));\n",
    "        \"\"\".format(mint = df2['mint'].values[0], hoard = df2['hoard'].values[0], weight = df2['weight'].values[0], start_date = df2['start_date'].values[0], end_date = df2['end_date'].values[0], geom = df2['geom'].values[0])\n",
    "    try:\n",
    "        cnx.execute(sqlInsert)\n",
    "    except:\n",
    "        print(\"error: {}\".format(sqlInsert))\n",
    "\n",
    "def typestoObverseCounter(mint):\n",
    "    sql = \"\"\"\n",
    "    Select typeid AS type, count(distinct obvdie) as number_of_obverse_dies\n",
    "    from\n",
    "    all_coins\n",
    "    where mint = '{mint}'\n",
    "    AND\n",
    "    obvdie IS NOT NULL\n",
    "    AND\n",
    "    obvdie != ''\n",
    "    AND\n",
    "    obvdie !='dupe'\n",
    "    AND\n",
    "    obvdie !='no_image'\n",
    "    AND\n",
    "    typeid IS NOT NULL\n",
    "    AND\n",
    "    typeid != ''\n",
    "    group by type\n",
    "    \"\"\".format(mint = mint)\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    df = pd.read_sql_query(sql, cnx)\n",
    "    \n",
    "    #we usually have mixed numbers and characters for naming conventions; this should arrange everythign nicely\n",
    "    df.index = df['type']\n",
    "    df = df.sort_index().reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def typescoincounts(mint):\n",
    "    sql = \"\"\"\n",
    "    Select typeid AS type, count(distinct id) as number_of_coins\n",
    "    from\n",
    "    all_coins\n",
    "    where mint = '{mint}'\n",
    "    AND\n",
    "    obvdie IS NOT NULL\n",
    "    AND\n",
    "    obvdie != ''\n",
    "    AND\n",
    "    obvdie !='dupe'\n",
    "    AND\n",
    "    obvdie !='no_image'\n",
    "    AND\n",
    "    typeid IS NOT NULL\n",
    "    AND\n",
    "    typeid != ''\n",
    "    group by typeid\n",
    "    \"\"\".format(mint = mint)\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    df = pd.read_sql_query(sql, cnx)\n",
    "    \n",
    "    #we usually have mixed numbers and characters for naming conventions; this should arrange everythign nicely\n",
    "    df.index = df['type']\n",
    "    df = df.sort_index().reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def typesweightaverager(mint):\n",
    "    sql = \"\"\"\n",
    "    Select typeid AS type, AVG(CAST(weight as decimal)) as average_weight\n",
    "    from\n",
    "    all_coins\n",
    "    where mint = '{mint}'\n",
    "    AND\n",
    "    obvdie IS NOT NULL\n",
    "    AND\n",
    "    obvdie != ''\n",
    "    AND\n",
    "    obvdie !='dupe'\n",
    "    AND\n",
    "    obvdie !='no_image'\n",
    "    AND\n",
    "    typeid IS NOT NULL\n",
    "    AND\n",
    "    typeid != ''\n",
    "    group by typeid\n",
    "    \"\"\".format(mint = mint)\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    df = pd.read_sql_query(sql, cnx)\n",
    "    \n",
    "    #we usually have mixed numbers and characters for naming conventions; this should arrange everythign nicely\n",
    "    df.index = df['type']\n",
    "    df = df.sort_index().reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def typeinfochart(mint):\n",
    "    dfObverseCount = pd.DataFrame()\n",
    "    dfObverseCount = typestoObverseCounter(mint)\n",
    "    dfCoinCount = pd.DataFrame()\n",
    "    dfCoinCount = typescoincounts(mint)\n",
    "    dfavgweight = pd.DataFrame()\n",
    "    dfavgweight = typesweightaverager(mint)\n",
    "    dfObvinfo = pd.DataFrame()\n",
    "    dfObvinfo = dfCoinCount.copy()\n",
    "    dfObvinfo = dfObvinfo.merge(dfObverseCount, how='left')\n",
    "    dfObvinfo = dfObvinfo.merge(dfavgweight, how='left')\n",
    "    dfObvinfo.set_index('type', inplace=True)\n",
    "    return dfObvinfo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pure SQL\n",
    "Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL to get the mints / hoards as source, target with counts for weights\n",
    "def coinNetworkSql(startdate, enddate, buffer):\n",
    "    sd = startdate - buffer;\n",
    "    ed = enddate - buffer;\n",
    "    sql = \"\"\"\n",
    "        SELECT  aa_mints.mint_uri as source, \n",
    "        aa_hoards.location_uri as target, \n",
    "        LOWER(aa_mints.mint) as label, \n",
    "        SUM(aa_mints.count) as weight\n",
    "    FROM\n",
    "    aa_mints\n",
    "    LEFT JOIN\n",
    "    aa_hoards\n",
    "    ON\n",
    "    aa_mints.hoard = aa_hoards.hoard_id\n",
    "    WHERE \n",
    "    CAST(b_start_date as decimal) >= {sd} and CAST(b_end_date as decimal) <= {ed}\n",
    "    AND\n",
    "    aa_hoards.hoard_id NOT IN (SELECT cross_reference FROM aa_parent_child)\n",
    "    AND\n",
    "    aa_mints.mint_uri IS NOT NULL \n",
    "    AND \n",
    "    aa_mints.mint_uri <> ''\n",
    "    AND aa_hoards.location_uri <> ''\n",
    "    GROUP BY source, target, label\n",
    "    ORDER BY label; \n",
    "    \"\"\".format(sd = sd, ed = ed)\n",
    "    return sql\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traditional Catalog\n",
    "\n",
    "In the notebook this only displays a selection; this can be exported to a csv to be styled and sorted as desired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "coinCatdf = pd.DataFrame()\n",
    "coinCatdf = coinCatalogWorking(mint[\"myrina\"])\n",
    "display(coinCatdf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export as a .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coinCatdf.to_csv('myrina_catalog.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List a Specific Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coinType = '1659'\n",
    "\n",
    "coinCatdf.loc[coinCatdf['type'] == coinType]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types and Weights as a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = coinCatdf.groupby(['type'])['weight'].mean()\n",
    "\n",
    "means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average Weight Per Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = means.plot(kind = 'bar')\n",
    "ax.set_ylabel('Mean Weight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histogram for all weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coinCatdf.hist(column='weight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histogram for weights by type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coinType = '1659'\n",
    "\n",
    "coinCatdf.loc[coinCatdf['type'] == coinType].hist(column='weight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count number of coins in a type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ax = coinCatdf['type'].value_counts().plot(kind='bar')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count number of reverse dies per obverse die"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As usual the logic is contained in a function above\n",
    "dfObverseReverseCount = pd.DataFrame()\n",
    "dfObverseReverseCount = obversetoreversecounter(mint[\"myrina\"])\n",
    "dfObverseReverseCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As usual the logic is contained in a function above\n",
    "dfobvprice = pd.DataFrame()\n",
    "\n",
    "dfobvprice = obverseToTypeList(mint[\"myrina\"])\n",
    "dfobvprice\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#average weights by die\n",
    "dfobvavgweight = pd.DataFrame()\n",
    "\n",
    "dfobvavgweight = obverseweightaverager(mint[\"myrina\"])\n",
    "dfobvavgweight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#average weights by die\n",
    "dfobvcoincount = pd.DataFrame()\n",
    "\n",
    "dfobvcoincount = obversecoincounter(mint[\"myrina\"])\n",
    "dfobvcoincount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, we can put these all together in whatever format is desired for viewing / publication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfObvinfo = pd.DataFrame()\n",
    "dfObvinfo = obvinfochart(mint[\"myrina\"])\n",
    "dfObvinfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can do the same for types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftypeinfo = pd.DataFrame()\n",
    "dftypeinfo = typeinfochart(mint[\"myrina\"])\n",
    "dftypeinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfObvinfo = pd.DataFrame()\n",
    "dfObvinfo = obvinfochart(mint[\"kyme\"])\n",
    "dfObvinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfObvinfo = pd.DataFrame()\n",
    "dfObvinfo = obvinfochart(mint[\"temnos\"])\n",
    "dfObvinfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coins as networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several different networks that can be created from our coin data. We can connect coins to types, dies (obverse and reverse) to types, coins to dies, and obverse to reverse dies. Each network type, with some different means of display and evaluation, are given below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obverse to types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coinCatdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coinGraph = nx.from_pandas_edgelist(coinCatdf, source='obverse', target='type')\n",
    "coinGraphDegree = dict(coinGraph.degree)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traditional bipartite graph with a twist: Circles are sized according to number of connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "\n",
    "\n",
    "pos = nx.bipartite_layout(coinGraph, coinCatdf['obverse'].tolist())\n",
    "\n",
    "nx.draw(coinGraph,pos, node_size=[v * 200 for v in coinGraphDegree.values()])\n",
    "\n",
    "nx.draw_networkx_labels(coinGraph,pos, font_size=20)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Die Linkages as a network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,12))\n",
    "\n",
    "pos = nx.nx_agraph.graphviz_layout(coinGraph, prog='neato')\n",
    "nx.draw(coinGraph, pos, node_size=[v * 100 for v in coinGraphDegree.values()])\n",
    "nx.draw_networkx_labels(coinGraph,pos,font_size=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obverse to Reverse Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coinGraph = nx.from_pandas_edgelist(coinCatdf, source='obverse', target='reverse')\n",
    "coinGraphDegree = dict(coinGraph.degree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "\n",
    "\n",
    "pos = nx.bipartite_layout(coinGraph, coinCatdf['obverse'].tolist())\n",
    "\n",
    "nx.draw(coinGraph,pos, node_size=[v * 200 for v in coinGraphDegree.values()])\n",
    "\n",
    "nx.draw_networkx_labels(coinGraph,pos, font_size=20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coins to Obverse Dies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coinGraph = nx.from_pandas_edgelist(coinCatdf, source='id', target='obverse')\n",
    "coinGraphDegree = dict(coinGraph.degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25,25))\n",
    "\n",
    "\n",
    "pos = nx.bipartite_layout(coinGraph, coinCatdf['obverse'].tolist())\n",
    "\n",
    "# Pass that layout to nx.draw\n",
    "nx.draw(coinGraph,pos, node_size=[v * 200 for v in coinGraphDegree.values()])\n",
    "\n",
    "nx.draw_networkx_labels(coinGraph,pos, font_size=20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Website"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use d3js and this data to create an application that allows users to sort the different coin networks, get information on the coins, and interact with the data which can be placed into any website that allows javascript. The following code will perform the required transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a nodes list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we must create a nodes list that has the following columns as nodes:\n",
    "1. Coins\n",
    "1. Obverse dies\n",
    "1. Reverse dies\n",
    "1. Coin types\n",
    "\n",
    "This will be a python dictionary that is later converted into a .json object. We will build this with SQL queries so it can be run live as the database changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The actual logic is contained in our function block; here we are showing how to export the results\n",
    "finalJson = makeHtmlJson(mint[\"kyme\"])\n",
    "\n",
    "text_file = open(\"kyme.json\", \"w\")\n",
    "text_file.write(finalJson)\n",
    "text_file.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The .json file can now be placed in the /data directory of the coin showcase application; for more information see https://github.com/Aeolian-Alexanders/website"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hoards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use these same tools and approaches to study hoards. We will first begin with ways to query hoards and perform statistical analysis; we will then move on to seeing how individual mints fit into an \"ego network\" of related hoards; and finally we will view all hoard data as a geospatial network using tools like PgRouting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hoard Information\n",
    "\n",
    "Totals are computed by the query; they are broken down by type and denomenation to enable more filtering or sorting if required. For die studies, it is advisable to filter based on types or denomenations. Although the current data is incomplete, it is my hope that inclusion in a future CoinHoards release by the ANS, along with making the data publicly accessable will spur further devlopment / interest in completing the records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = -282\n",
    "end_date = -129\n",
    "buffer = 2\n",
    "\n",
    "dfHoard = pd.DataFrame()\n",
    "dfHoard = hoardsDisplay (mint[\"myrina\"], start_date, end_date, buffer)\n",
    "dfHoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic histogram of number of coins found in each hoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfHoard.hist(column='count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map all Hoards associated with a mint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logic is contained within the function; this is showing how it is called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MinimumMarkerSize = 5\n",
    "MaximumMarkerSize = 25\n",
    "\n",
    "m = createmapvis()\n",
    "mintHoardMapper(dfHoard, \"count\", (mint[\"myrina\"]), MinimumMarkerSize, MaximumMarkerSize, m)\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hoards as Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to model hoards as networks; the mints and hoards will be nodes connecting to each other, with the number of coins of each mint as the weight. We will begin with an ego network showing just the mints that are associated with our mint of interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an ego network surrounding the mint of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = -282\n",
    "end_date = -129\n",
    "buffer = 2\n",
    "\n",
    "dfEgoHoard = pd.DataFrame()\n",
    "dfEgoHoard = hoardsEgoDisplay (mint[\"myrina\"], start_date, end_date, buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfEgoHoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this is nice for seeing the data in a tabular form, we will need to do some woek on the table and the graph before we display it. What we will do is get network statistics for the graph, THEN break it into different dataframes which are brought into a new graph for display. We want to set our colors based on the partition, and then keep those same colors for mapping the nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will create our ego network by splitting the data frame, and run all of the partition / modularity off of it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to get the weight of our edges, so the foloowing code pulls data form our main list and forms a new dataframe with the weight of the conecction derived from the total number of coins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfEgoHoardEdge = dfEgoHoard.groupby(['mint_uri', 'hoard_id',]).sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfEgoHoardEdge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This may seem counter-intuitive, but we can use a \"staging\" graph to break apart our main table into nodes, which we then build another dataframe around. We are doing it this way to pull less data back and forth from the database. We are going to peform som network analysis on this, then pull apart the nodes with properties that can be used in folium and other libraries without having to worry about changing colors, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coinHoardGraph = nx.from_pandas_edgelist(dfEgoHoardEdge, 'mint_uri', 'hoard_id','count')\n",
    "coinHoardGraphDict = dict(coinHoardGraph.degree)\n",
    "partition = community.best_partition(coinHoardGraph)\n",
    "modularity = community.modularity(partition, coinHoardGraph)\n",
    "\n",
    "colors = [partition[n] for n in coinHoardGraph.nodes()]\n",
    "my_colors = plt.cm.Set2 # you can select other color pallettes here: https://matplotlib.org/users/colormaps.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, rebuild the nodes and edges tables from the network. We will use these to build our maps, etc. We are also going to have to split the hoard and the mints to make this work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframeResults = {}\n",
    "dataframeResults = createMapDfs(dfEgoHoard, partition, coinHoardGraph, my_colors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hoards dataframe contains information about hoards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframeResults['hoardsDf']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mints dataframe contains information about mints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframeResults['mintsDf']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The map dataframe ties together everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframeResults['mapDf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displayCoinEgoHoardGraph (coinHoardGraph, coinHoardGraphDict, 30, 5, 4, .5, 20, 20, dataframeResults['mapDf'], 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using our function, we can map all hoards and mints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = createmapvis()\n",
    "\n",
    "#we are going to copy the dataframe so we can change the color\n",
    "\n",
    "hoardsinputdf = dataframeResults['mapDf'].copy()\n",
    "hoardsinputdf['color'] = hoardsinputdf.apply (lambda row: makeNetColor(row, my_colors), axis=1)\n",
    "\n",
    "hoardsdf = pd.DataFrame()\n",
    "hoardsfg = folium.FeatureGroup(name='Coin Hoards Network: {mint}'.format(mint = 'myrina' ))\n",
    "addFeatureGrouptoMap(hoardsinputdf, hoardsfg, 30, 5, 'degree', 'title', m)\n",
    "#optional labels - not really necessary when the map is interactive\n",
    "addFeatureLabelstoMap(hoardsinputdf, 'title', m, 5)\n",
    "m.add_child(folium.LayerControl())\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or just the mints / hoards as desired:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "m = createmapvis()\n",
    "\n",
    "#we are going to copy the dataframe so we can change the color\n",
    "\n",
    "hoardsinputdf = dataframeResults['mintsDf'].copy()\n",
    "hoardsinputdf['color'] = hoardsinputdf.apply (lambda row: makeNetColor(row, my_colors), axis=1)\n",
    "\n",
    "hoardsdf = pd.DataFrame()\n",
    "hoardsfg = folium.FeatureGroup(name='Coin Hoards Network: {mint}'.format(mint = 'myrina' ))\n",
    "addFeatureGrouptoMap(hoardsinputdf, hoardsfg, 30, 5, 'degree', 'title', m)\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All hoards at a given time in a network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the tools and data we created to see all mints that are represented in hoards between any arbitrary diates. The idea here is to see how the partitions (communities) changed over time.\n",
    "\n",
    "We can also map mints that ANS MANTIS identiifed as belonging to a certain dynasty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startdate = third_syrian_war['start']\n",
    "enddate = third_syrian_war['end']\n",
    "buffer = 5\n",
    "\n",
    "m = createmapvis()\n",
    "\n",
    "hoardsdf = pd.DataFrame()\n",
    "hoardsfg = folium.FeatureGroup(name='Coin Hoards Network: {startdate} to {enddate}'.format(startdate = startdate - buffer,enddate = enddate + buffer ))\n",
    "hoardsdf = hoardsGroupMapper(startdate, enddate, buffer, my_colors, hoardsfg)\n",
    "addFeatureGrouptoMap(hoardsdf, hoardsfg, 30, 5, 'degree', 'title', m)\n",
    "\n",
    "ptolemyfg = folium.FeatureGroup(name='Ptolemaic Mints')\n",
    "ptolemydf = geopandas.GeoDataFrame()\n",
    "ptolemydf = dynastymintlocator(startdate, enddate, 'ptolem')\n",
    "dynastyMintMapper(ptolemydf,'orange', 5, ptolemyfg, m)\n",
    "\n",
    "\n",
    "seleucidfg = folium.FeatureGroup(name='Seleucid Mints')\n",
    "seleuciddf = geopandas.GeoDataFrame()\n",
    "seleuciddf = dynastymintlocator(startdate, enddate, 'seleuc')\n",
    "dynastyMintMapper(ptolemydf,'black', 5, seleucidfg, m)\n",
    "\n",
    "m.add_child(folium.LayerControl())\n",
    "m\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Abstract Route Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTE: These will ONLY work in a postgre/postgis databse! They are here for reference, and to serve as a model for further exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can abstract the flow of coins to hoards by using the pgRouting library, along with different LOD resources (in our case routes from ORBIS, roads from AWMC/DARMC, and roads from Harvard World Map). The idea here is to create a visual \"flow\" diagram that illustrates quantities of coins moving through economic networks. As the coins almost certainly did not go straight from the mint to a hoard, this is a visual abstrction that attempts to see potentially important routes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This first function makes a route network from a given ego to hoards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfHoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gdf = geopandas.GeoDataFrame()\n",
    "gdf = polityRouteMaker (dfHoard, 'location_uri', 'aa_locations', 'places', mint[\"kyme\"])\n",
    "gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should be mapped; a basic exmaple is given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = createmapvis()\n",
    "\n",
    "folium.Choropleth(\n",
    "    gdf,\n",
    "    line_weight=3,\n",
    "    line_color='blue'\n",
    ").add_to(m)\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These routes can be added to our exsisting maps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = createmapvis()\n",
    "\n",
    "#we are going to copy the dataframe so we can change the color\n",
    "\n",
    "hoardsinputdf = dataframeResults['hoardsDf'].copy()\n",
    "hoardsinputdf['color'] = hoardsinputdf.apply (lambda row: makeNetColor(row, my_colors), axis=1)\n",
    "\n",
    "hoardsdf = pd.DataFrame()\n",
    "hoardsfg = folium.FeatureGroup(name='Coin Hoards Network: {mint}'.format(mint = 'myrina' ))\n",
    "addFeatureGrouptoMap(hoardsinputdf, hoardsfg, 30, 5, 'degree', 'title', m)\n",
    "\n",
    "folium.Choropleth(\n",
    "    gdf,\n",
    "    line_weight=3,\n",
    "    line_color='blue'\n",
    ").add_to(m)\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To continue studying routes as networks, we can build this for *all* mints and hoards in the database. This is far too complex to display in a jupyter/folium combination, but the data can be brought into QGIS or other GIS desktop software. To facilitate this, we are also changing the dates so they can be read by the QGIS timeseries plugin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions for doing so are shown below; these will modify the data on your machine. The intent is to show the process and data structures, so they can be applied to different investigations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will show the big dataaframe here to get an idea of all of the mints to hoards connections that we can map. This will grow as more mints are assigned places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df = createBigNetworkDf()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beyond Coins: Mapping Ancient Polities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The routes idea from the hoards can be scaled up to look at probable routes and axes of communication for larger political entities. In this case, we are taking gazetteers of connections (political, social, etc) uncovered by this investigation, plotting the known places mentioned in the gazetteers, and then constructing route lines like we did between mints and hoards. This allows us to visualize and consider the political (and social) networks that were also at play at the same time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our example, we will look at Pergamon during its earliest times, then its larger extent after the peace of Apamea in 188 BCE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polity = 550812\n",
    "\n",
    "df = ancientEmpireRoutes(polity, 'aa_hellenistic_edges', -262, -197)\n",
    "dfpoints = ancientEmpirePoints(polity, 'aa_hellenistic_edges', -262, -197)\n",
    "\n",
    "dfRoutes = gpd.GeoDataFrame = polityRouteMaker (df, 'id', 'places', 'places', polity )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = createmapvis()\n",
    "\n",
    "folium.Choropleth(\n",
    "    dfRoutes,\n",
    "    line_weight=3,\n",
    "    line_color='red'\n",
    ").add_to(m)\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polity = 550812\n",
    "\n",
    "df = ancientEmpireRoutes(polity, 'aa_hellenistic_edges', -190, -128)\n",
    "dfpoints = ancientEmpirePoints(polity, 'aa_hellenistic_edges', -190, -128)\n",
    "\n",
    "dfRoutes = gpd.GeoDataFrame = polityRouteMaker (df, 'id', 'places', 'places', polity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = createmapvis()\n",
    "\n",
    "folium.Choropleth(\n",
    "    dfRoutes,\n",
    "    line_weight=3,\n",
    "    line_color='red'\n",
    ").add_to(m)\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to limitations in folium, it is currently nearlly impossible to style the lines by the number of times they are crossed by routes; we can do this with another library but we can not use the accurate backround. As an alternative, we can bring these routes into QGIS by simply performing the same queries there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = dfRoutes.plot(figsize=(10, 10), alpha=0.5, edgecolor='k', linewidth=dfRoutes['count'])\n",
    "ctx.add_basemap(ax, crs=dfRoutes.crs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This higlights an important aspect of the view: potentially important areas are crossed by more routes, leading to larger lines. We can combine these to see a network of coins against a political entitiy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = createmapvis()\n",
    "\n",
    "folium.Choropleth(\n",
    "    dfRoutes,\n",
    "    line_weight=3,\n",
    "    line_color='red'\n",
    ").add_to(m)\n",
    "\n",
    "folium.Choropleth(\n",
    "    gdf,\n",
    "    line_weight=3,\n",
    "    line_color='blue'\n",
    ").add_to(m)\n",
    "\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the routes show that Myrina's coinage stretchedd far beyond the confines of the Attalid kingdom, and is far more oriented to the heartland of the Seleucid Empire, which previously dominated the area."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Tying it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, what about aeolian economic activity during some of the most intensive production of Temnos? We can see the silver flows from Temnos when compared with the axes of communication of the Attalids and the diminshed Seleukid Empire from 153 to 145 BCE. We only have limited data on when certian communites changed hands; this project was overly conservative when creating the networks, and only considered cases where imperial power was explicitly revealed in literature, epigraphy, or papyri. Once again this data can be steadily improved with more community involvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = -153\n",
    "end_date = -145\n",
    "buffer = 2\n",
    "\n",
    "m = createmapvis()\n",
    "\n",
    "dfEgoHoard = pd.DataFrame()\n",
    "dfEgoHoard = hoardsEgoDisplay (mint[\"temnos\"], start_date, end_date, buffer)\n",
    "dfEgoHoardEdge = dfEgoHoard.groupby(['mint_uri', 'hoard_id',]).sum().reset_index()\n",
    "\n",
    "coinHoardGraph = nx.from_pandas_edgelist(dfEgoHoardEdge, 'mint_uri', 'hoard_id','count')\n",
    "coinHoardGraphDict = dict(coinHoardGraph.degree)\n",
    "partition = community.best_partition(coinHoardGraph)\n",
    "modularity = community.modularity(partition, coinHoardGraph)\n",
    "\n",
    "colors = [partition[n] for n in coinHoardGraph.nodes()]\n",
    "my_colors = plt.cm.Set2 # you can select other color pallettes here: https://matplotlib.org/users/colormaps.html\n",
    "\n",
    "dataframeResults = {}\n",
    "dataframeResults = createMapDfs(dfEgoHoard, partition, coinHoardGraph, my_colors)\n",
    "\n",
    "\n",
    "hoardsinputdf = dataframeResults['hoardsDf'].copy()\n",
    "hoardsinputdf['color'] = hoardsinputdf.apply (lambda row: makeNetColor(row, my_colors), axis=1)\n",
    "\n",
    "hoardsdf = pd.DataFrame()\n",
    "hoardsfg = folium.FeatureGroup(name='Coin Hoards Network: {mint}'.format(mint = 'temnos' ))\n",
    "addFeatureGrouptoMap(hoardsinputdf, hoardsfg, 30, 5, 'degree', 'title', m)\n",
    "\n",
    "gdfTemnos = polityRouteMaker (dfEgoHoard, 'location_uri', 'aa_locations', 'places', mint[\"temnos\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "folium.Choropleth(\n",
    "    gdfTemnos,\n",
    "    line_weight=3,\n",
    "    line_color='black'\n",
    ").add_to(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = -153\n",
    "end_date = -145\n",
    "buffer = 2\n",
    "\n",
    "m = createmapvis()\n",
    "\n",
    "dfEgoHoard = pd.DataFrame()\n",
    "dfEgoHoard = hoardsEgoDisplay (mint[\"myrina\"], start_date, end_date, buffer)\n",
    "dfEgoHoardEdge = dfEgoHoard.groupby(['mint_uri', 'hoard_id',]).sum().reset_index()\n",
    "\n",
    "coinHoardGraph = nx.from_pandas_edgelist(dfEgoHoardEdge, 'mint_uri', 'hoard_id','count')\n",
    "coinHoardGraphDict = dict(coinHoardGraph.degree)\n",
    "partition = community.best_partition(coinHoardGraph)\n",
    "modularity = community.modularity(partition, coinHoardGraph)\n",
    "\n",
    "colors = [partition[n] for n in coinHoardGraph.nodes()]\n",
    "my_colors = plt.cm.Set2 # you can select other color pallettes here: https://matplotlib.org/users/colormaps.html\n",
    "\n",
    "dataframeResults = {}\n",
    "dataframeResults = createMapDfs(dfEgoHoard, partition, coinHoardGraph, my_colors)\n",
    "\n",
    "\n",
    "hoardsinputdf = dataframeResults['hoardsDf'].copy()\n",
    "hoardsinputdf['color'] = hoardsinputdf.apply (lambda row: makeNetColor(row, my_colors), axis=1)\n",
    "\n",
    "hoardsdf = pd.DataFrame()\n",
    "hoardsfg = folium.FeatureGroup(name='Coin Hoards Network: {mint}'.format(mint = 'myrina' ))\n",
    "addFeatureGrouptoMap(hoardsinputdf, hoardsfg, 30, 5, 'degree', 'title', m)\n",
    "\n",
    "gdfmyrina = polityRouteMaker (dfEgoHoard, 'location_uri', 'aa_locations', 'places', mint[\"myrina\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folium.Choropleth(\n",
    "    gdfmyrina,\n",
    "    line_weight=3,\n",
    "    line_color='orange'\n",
    ").add_to(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = -153\n",
    "end_date = -145\n",
    "buffer = 2\n",
    "\n",
    "m = createmapvis()\n",
    "\n",
    "dfEgoHoard = pd.DataFrame()\n",
    "dfEgoHoard = hoardsEgoDisplay (mint[\"kyme\"], start_date, end_date, buffer)\n",
    "dfEgoHoardEdge = dfEgoHoard.groupby(['mint_uri', 'hoard_id',]).sum().reset_index()\n",
    "\n",
    "coinHoardGraph = nx.from_pandas_edgelist(dfEgoHoardEdge, 'mint_uri', 'hoard_id','count')\n",
    "coinHoardGraphDict = dict(coinHoardGraph.degree)\n",
    "partition = community.best_partition(coinHoardGraph)\n",
    "modularity = community.modularity(partition, coinHoardGraph)\n",
    "\n",
    "colors = [partition[n] for n in coinHoardGraph.nodes()]\n",
    "my_colors = plt.cm.Set2 # you can select other color pallettes here: https://matplotlib.org/users/colormaps.html\n",
    "\n",
    "dataframeResults = {}\n",
    "dataframeResults = createMapDfs(dfEgoHoard, partition, coinHoardGraph, my_colors)\n",
    "\n",
    "\n",
    "hoardsinputdf = dataframeResults['hoardsDf'].copy()\n",
    "hoardsinputdf['color'] = hoardsinputdf.apply (lambda row: makeNetColor(row, my_colors), axis=1)\n",
    "\n",
    "hoardsdf = pd.DataFrame()\n",
    "hoardsfg = folium.FeatureGroup(name='Coin Hoards Network: {mint}'.format(mint = 'kyme' ))\n",
    "addFeatureGrouptoMap(hoardsinputdf, hoardsfg, 30, 5, 'degree', 'title', m)\n",
    "\n",
    "gdfkyme = polityRouteMaker (dfEgoHoard, 'location_uri', 'aa_locations', 'places', mint[\"kyme\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folium.Choropleth(\n",
    "    gdfkyme,\n",
    "    line_weight=3,\n",
    "    line_color='yellow'\n",
    ").add_to(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polity = 550812\n",
    "\n",
    "dfAttalid = ancientEmpireRoutes(polity, 'aa_hellenistic_edges', -153, -145)\n",
    "dfAttalidPoints = ancientEmpirePoints(polity, 'aa_hellenistic_edges', -153, -145)\n",
    "\n",
    "dfAttalidRoutes = gpd.GeoDataFrame = polityRouteMaker (dfAttalid, 'id', 'places', 'places', polity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polity = 45635759\n",
    "\n",
    "dfSeleukid = ancientEmpireRoutes(polity, 'aa_hellenistic_edges', -153, -145)\n",
    "dfSeleukidPoints = ancientEmpirePoints(polity, 'aa_hellenistic_edges', -153, -145)\n",
    "\n",
    "dfSeleukidRoutes = gpd.GeoDataFrame = polityRouteMaker (dfSeleukid, 'id', 'places', 'places', polity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "folium.Choropleth(\n",
    "    dfAttalidRoutes,\n",
    "    line_weight=3,\n",
    "    line_color='red'\n",
    ").add_to(m)\n",
    "\n",
    "folium.Choropleth(\n",
    "    dfSeleukidRoutes,\n",
    "    line_weight=3,\n",
    "    line_color='blue'\n",
    ").add_to(m)\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the data behind this map can be extensively explored in jupyter, it can also be exported to QGIS and styled there (and touched up in inkscape)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdfTemnos.to_file('temnos_routes', driver=\"GeoJSON\")\n",
    "gdfmyrina.to_file('myrina_routes', driver=\"GeoJSON\")\n",
    "gdfkyme.to_file('kyme_routes', driver=\"GeoJSON\")\n",
    "\n",
    "dfAttalidRoutes.to_file('attalid_routes', driver=\"GeoJSON\")\n",
    "dfSeleukidRoutes.to_file('seleukid_routes', driver=\"GeoJSON\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <img src=\"img/balas_mints.png\" alt=\"Networks surrounding Alexander Balas\" style=\"width: 1000px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "conn = sqlite3.connect(\"https://github.com/Aeolian-Alexanders/data/blob/master/sqlite%20database/aeolian_alexanders.db?raw=true\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur = conn.cursor()\n",
    "cur.execute(\"select * from all_coins limit 5;\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
